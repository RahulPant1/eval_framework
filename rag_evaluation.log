2025-04-08 20:17:12,618 - INFO - Setting up database connection...
2025-04-08 20:17:12,620 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:17:12,620 - INFO - Creating collections...
2025-04-08 20:17:12,622 - INFO - Collection 'llms' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'llm_endpoints' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'model_deployments' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'datasets' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'prompt_templates' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'synthetic_data' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'evaluation_harnesses' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'test_suites' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'evaluation_runs' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'test_results' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'metrics' already exists and will be reused.
2025-04-08 20:17:12,622 - INFO - Collection 'metrics_definitions' already exists and will be reused.
2025-04-08 20:17:12,623 - INFO - Collection 'vector_store' already exists and will be reused.
2025-04-08 20:17:12,623 - INFO - Inserting initial data...
2025-04-08 20:17:12,623 - INFO - LLMs collection already has data, skipping insertion.
2025-04-08 20:17:12,623 - INFO - LLM endpoints collection already has data, skipping insertion.
2025-04-08 20:17:12,624 - INFO - Model deployments collection already has data, skipping insertion.
2025-04-08 20:17:12,624 - INFO - Vector store collection already has data, skipping insertion.
2025-04-08 20:17:12,624 - INFO - Evaluation harnesses collection already has data, skipping insertion.
2025-04-08 20:17:12,625 - INFO - Test suites collection already has data, skipping insertion.
2025-04-08 20:17:12,625 - INFO - Database initialization completed successfully!
2025-04-08 20:17:27,042 - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-04-08 20:17:27,042 - INFO - NumExpr defaulting to 16 threads.
2025-04-08 20:17:27,267 - INFO - PyTorch version 2.3.1 available.
2025-04-08 20:17:29,444 - INFO - Setting up database connection...
2025-04-08 20:17:29,445 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:17:29,445 - INFO - Loading data from sample_data.csv...
2025-04-08 20:17:29,450 - INFO - Loaded 3 chunks from CSV file: sample_data.csv
2025-04-08 20:17:29,451 - INFO - Dataset stored with ID: 67f53701e7145902639f90ba
2025-04-08 20:17:29,451 - INFO - Adding chunks to vector store...
2025-04-08 20:17:29,452 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:17:29,454 - INFO - Use pytorch device_name: cuda
2025-04-08 20:17:29,454 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:17:33,285 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-04-08 20:17:33,362 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:17:33,938 - INFO - Chunks successfully added to vector store
2025-04-08 20:17:33,938 - INFO - Generating synthetic QA pairs using Gemini 1.5 Flash...
2025-04-08 20:17:33,940 - INFO - Prompt sent to Gemini model: Generate 2 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 20:17:36,377 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What is the Retrieval-Augmented Generation (RAG) approach in natural language processing?\nAnswer: RAG combines retrieval-based and generation-based methods.  It uses a retrieval component to fetch relevant information from a knowledge base, which a generative model then uses to create more accurate and informative responses.\n\nQuestion: How does RAG address the limitations of large language models?\nAnswer: RAG addresses these limitations by grounding the generation of responses in external, up-to-date knowledge sources, rather than relying solely on the model's internal knowledge.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.08010158707610274
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 113,
        "total_token_count": 232
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:17:36,377 - INFO - Final extracted text: Question: What is the Retrieval-Augmented Generation (RAG) approach in natural language processing?
Answer: RAG combines retrieval-based and generation-based methods.  It uses a retrieval component to fetch relevant information from a knowledge base, which a generative model then uses to create more accurate and informative responses.

Question: How does RAG address the limitations of large language models?
Answer: RAG addresses these limitations by grounding the generation of responses in external, up-to-date knowledge sources, rather than relying solely on the model's internal knowledge.

2025-04-08 20:17:36,378 - INFO - Successfully parsed 2 QA pairs.
2025-04-08 20:17:36,379 - INFO - Prompt sent to Gemini model: Generate 2 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:17:37,373 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What type of data do vector databases store and query?\nAnswer: Vector databases store and efficiently query high-dimensional vector embeddings, which represent semantic information about text, images, or other data types in a numerical format.\n\nQuestion:  Name some popular examples of vector databases.\nAnswer: Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.026882683789288556
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 81,
        "total_token_count": 200
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:17:37,374 - INFO - Final extracted text: Question: What type of data do vector databases store and query?
Answer: Vector databases store and efficiently query high-dimensional vector embeddings, which represent semantic information about text, images, or other data types in a numerical format.

Question:  Name some popular examples of vector databases.
Answer: Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB.

2025-04-08 20:17:37,374 - INFO - Successfully parsed 2 QA pairs.
2025-04-08 20:17:37,375 - INFO - Prompt sent to Gemini model: Generate 2 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query.
2025-04-08 20:17:38,928 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?\nAnswer: RAG systems are evaluated using retrieval-focused metrics (e.g., precision, recall, MRR) and generation-focused metrics (e.g., BLEU, ROUGE, METEOR), as well as RAG-specific metrics like faithfulness and relevance.\n\nQuestion:  How do retrieval-focused metrics differ from generation-focused metrics in evaluating RAG systems?\nAnswer: Retrieval-focused metrics assess the quality of the retrieved documents (e.g., how relevant they are), while generation-focused metrics evaluate the quality of the generated text itself by comparing it to reference answers.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.03445120338055727
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 126,
        "candidates_token_count": 139,
        "total_token_count": 265
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:17:38,928 - INFO - Final extracted text: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?
Answer: RAG systems are evaluated using retrieval-focused metrics (e.g., precision, recall, MRR) and generation-focused metrics (e.g., BLEU, ROUGE, METEOR), as well as RAG-specific metrics like faithfulness and relevance.

Question:  How do retrieval-focused metrics differ from generation-focused metrics in evaluating RAG systems?
Answer: Retrieval-focused metrics assess the quality of the retrieved documents (e.g., how relevant they are), while generation-focused metrics evaluate the quality of the generated text itself by comparing it to reference answers.

2025-04-08 20:17:38,928 - INFO - Successfully parsed 2 QA pairs.
2025-04-08 20:17:38,930 - INFO - Generated 6 QA pairs
2025-04-08 20:17:38,931 - INFO - Creating test suite...
2025-04-08 20:17:38,931 - INFO - Test suite created with ID: 67f5370ae7145902639f90c2
2025-04-08 20:17:38,932 - INFO - Running evaluation with Gemini 1.5 Flash...
2025-04-08 20:17:38,935 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:17:38,941 - INFO - Use pytorch device_name: cuda
2025-04-08 20:17:38,941 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:17:41,682 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:17:41,714 - INFO - Calling generate_answers method
2025-04-08 20:17:41,714 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What is the Retrieval-Augmented Generation (RAG) approach in natural language processing?

Context: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:17:44,051 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Retrieval-Augmented Generation (RAG) is a natural language processing approach that combines retrieval-based and generation-based methods.  It works by first using a retrieval component to find relevant documents or passages from a knowledge base.  Then, a generative model uses this retrieved information to create more accurate and informative responses.  This addresses limitations of large language models by grounding their output in external, current knowledge sources.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.07021693149244929
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 302,
        "candidates_token_count": 83,
        "total_token_count": 385
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:17:45,680 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:17:45,683 - INFO - Use pytorch device_name: cuda
2025-04-08 20:17:45,683 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:17:48,369 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:17:48,399 - INFO - Calling generate_answers method
2025-04-08 20:17:48,399 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: How does RAG address the limitations of large language models?

Context: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:17:50,735 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "RAG addresses the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.  This contrasts with LLMs which may hallucinate or produce inaccurate information due to limitations in their training data.  By retrieving relevant information from a knowledge base, RAG ensures the LLM's responses are more accurate and informative.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.05662721906389509
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 296,
        "candidates_token_count": 70,
        "total_token_count": 366
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:17:50,742 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:17:50,745 - INFO - Use pytorch device_name: cuda
2025-04-08 20:17:50,745 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:17:53,290 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:17:53,305 - INFO - Calling generate_answers method
2025-04-08 20:17:53,305 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What type of data do vector databases store and query?

Context: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 20:17:55,238 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text, vector databases store and query **high-dimensional vector embeddings**.  These embeddings represent semantic information about data such as text and images in a numerical format.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.11907760517017262
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 296,
        "candidates_token_count": 37,
        "total_token_count": 333
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:17:55,246 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:17:55,251 - INFO - Use pytorch device_name: cuda
2025-04-08 20:17:55,251 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:17:57,961 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:17:57,969 - INFO - Calling generate_answers method
2025-04-08 20:17:57,970 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: Name some popular examples of vector databases.

Context: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 20:17:59,865 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text, some popular examples of vector databases are Pinecone, Weaviate, Milvus, and ChromaDB.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.027403773932621396
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 293,
        "candidates_token_count": 29,
        "total_token_count": 322
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:17:59,868 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:17:59,871 - INFO - Use pytorch device_name: cuda
2025-04-08 20:17:59,871 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:18:02,565 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:18:02,585 - INFO - Calling generate_answers method
2025-04-08 20:18:02,585 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?

Context: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:18:03,810 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Evaluation metrics for RAG systems fall into three categories:\n\n1. **Retrieval-focused metrics:** These assess the quality of the retrieved documents. Examples include precision, recall, and mean reciprocal rank (MRR).\n\n2. **Generation-focused metrics:** These evaluate the quality of the generated text. Examples include BLEU, ROUGE, and METEOR.\n\n3. **RAG-specific metrics:** These measure how well the generated response uses the retrieved information and addresses the original query. Examples include faithfulness and relevance.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.050405779339018325
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 301,
        "candidates_token_count": 105,
        "total_token_count": 406
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:18:03,823 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:18:03,826 - INFO - Use pytorch device_name: cuda
2025-04-08 20:18:03,826 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:18:06,442 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:18:06,458 - INFO - Calling generate_answers method
2025-04-08 20:18:06,458 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: How do retrieval-focused metrics differ from generation-focused metrics in evaluating RAG systems?

Context: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:18:09,561 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Retrieval-focused metrics (like precision, recall, and MRR) evaluate the *effectiveness of the retrieval component* in finding relevant documents from the knowledge base.  Generation-focused metrics (like BLEU, ROUGE, and METEOR), on the other hand, assess the *quality of the text generated* by the model, comparing it to a reference answer.  In short, retrieval metrics focus on the accuracy of document selection, while generation metrics focus on the quality of the answer synthesized from those documents.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.10254196020273063
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 302,
        "candidates_token_count": 104,
        "total_token_count": 406
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:18:09,579 - INFO - Evaluation completed with 6 results
2025-04-08 20:18:09,589 - INFO - Generated metrics reports: reports/rag_evaluation_report_20250408_201809.html
2025-04-08 20:18:09,589 - INFO - Generated metrics report at: {'html': 'reports/rag_evaluation_report_20250408_201809.html'}
2025-04-08 20:23:48,745 - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-04-08 20:23:48,745 - INFO - NumExpr defaulting to 16 threads.
2025-04-08 20:23:48,874 - INFO - PyTorch version 2.3.1 available.
2025-04-08 20:23:50,007 - INFO - Setting up database connection...
2025-04-08 20:23:50,009 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:23:50,009 - INFO - Loading data from sample_data.csv...
2025-04-08 20:23:50,011 - INFO - Loaded 3 chunks from CSV file: sample_data.csv
2025-04-08 20:23:50,013 - INFO - Dataset stored with ID: 67f5387e1d8b6c71c4e3b7e3
2025-04-08 20:23:50,013 - INFO - Adding chunks to vector store...
2025-04-08 20:23:50,014 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:23:50,017 - INFO - Use pytorch device_name: cuda
2025-04-08 20:23:50,017 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:23:53,121 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-04-08 20:23:53,174 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:23:53,542 - WARNING - Insert of existing embedding ID: 0
2025-04-08 20:23:53,542 - WARNING - Insert of existing embedding ID: 1
2025-04-08 20:23:53,543 - WARNING - Insert of existing embedding ID: 2
2025-04-08 20:23:53,543 - WARNING - Add of existing embedding ID: 0
2025-04-08 20:23:53,543 - WARNING - Add of existing embedding ID: 1
2025-04-08 20:23:53,543 - WARNING - Add of existing embedding ID: 2
2025-04-08 20:23:53,547 - INFO - Chunks successfully added to vector store
2025-04-08 20:23:53,547 - INFO - Generating synthetic QA pairs using Gemini 1.5 Flash...
2025-04-08 20:23:53,549 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 20:23:55,706 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?\nAnswer: RAG enhances large language models by grounding their responses in external, up-to-date knowledge sources retrieved from a knowledge base, leading to more accurate and informative outputs, unlike models that solely rely on their internal training data.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.14858892689580502
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 69,
        "total_token_count": 188
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:23:55,707 - INFO - Final extracted text: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?
Answer: RAG enhances large language models by grounding their responses in external, up-to-date knowledge sources retrieved from a knowledge base, leading to more accurate and informative outputs, unlike models that solely rely on their internal training data.

2025-04-08 20:23:55,708 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 20:23:55,710 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:23:56,611 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What are vector databases and what types of data do they handle?\nAnswer: Vector databases are specialized database systems designed to efficiently store and query high-dimensional vector embeddings representing semantic information from text, images, or other data types.  They capture the meaning and relationships of this data in a numerical format.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.12198539078235626
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 64,
        "total_token_count": 183
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:23:56,611 - INFO - Final extracted text: Question: What are vector databases and what types of data do they handle?
Answer: Vector databases are specialized database systems designed to efficiently store and query high-dimensional vector embeddings representing semantic information from text, images, or other data types.  They capture the meaning and relationships of this data in a numerical format.

2025-04-08 20:23:56,611 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 20:23:56,613 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query.
2025-04-08 20:23:58,006 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems, and what aspects of the system do they assess?\nAnswer: RAG systems are evaluated using retrieval-focused metrics (precision, recall, MRR) which assess the relevance of retrieved documents, and generation-focused metrics (BLEU, ROUGE, METEOR) which evaluate the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance measure how well the generated response uses the retrieved information and answers the query.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.03615522384643555
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 126,
        "candidates_token_count": 107,
        "total_token_count": 233
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:23:58,006 - INFO - Final extracted text: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems, and what aspects of the system do they assess?
Answer: RAG systems are evaluated using retrieval-focused metrics (precision, recall, MRR) which assess the relevance of retrieved documents, and generation-focused metrics (BLEU, ROUGE, METEOR) which evaluate the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance measure how well the generated response uses the retrieved information and answers the query.

2025-04-08 20:23:58,006 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 20:23:58,009 - INFO - Generated 3 QA pairs
2025-04-08 20:23:58,009 - INFO - Creating test suite...
2025-04-08 20:23:58,009 - INFO - Test suite created with ID: 67f538861d8b6c71c4e3b7e8
2025-04-08 20:23:58,009 - INFO - Running evaluation with Gemini 1.5 Flash...
2025-04-08 20:23:58,013 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:23:58,016 - INFO - Use pytorch device_name: cuda
2025-04-08 20:23:58,016 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:24:00,835 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:24:00,854 - INFO - Calling generate_answers method
2025-04-08 20:24:00,854 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?

Context: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:24:03,245 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources.  Instead of relying solely on the model's internal knowledge (which may be outdated or incomplete), RAG first retrieves relevant information from a knowledge base, allowing the generative model to produce more accurate and informative responses.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.04099128062908466
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 304,
        "candidates_token_count": 65,
        "total_token_count": 369
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:24:04,807 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:24:04,809 - INFO - Use pytorch device_name: cuda
2025-04-08 20:24:04,809 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:24:08,404 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:24:08,420 - INFO - Calling generate_answers method
2025-04-08 20:24:08,421 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What are vector databases and what types of data do they handle?

Context: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 20:24:11,256 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text, vector databases are specialized database systems that store and efficiently query high-dimensional vector embeddings.  These embeddings are numerical representations of semantic information from data like text and images, capturing their meaning and relationships.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.05684045086736265
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 298,
        "candidates_token_count": 46,
        "total_token_count": 344
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:24:11,267 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:24:11,270 - INFO - Use pytorch device_name: cuda
2025-04-08 20:24:11,270 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:24:13,933 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:24:13,950 - INFO - Calling generate_answers method
2025-04-08 20:24:13,951 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems, and what aspects of the system do they assess?

Context: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:24:17,286 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Evaluation metrics for RAG systems are categorized into retrieval-focused and generation-focused metrics, plus RAG-specific metrics.\n\n* **Retrieval-focused metrics:**  These assess the effectiveness of the retrieval component.  Examples include precision, recall, and mean reciprocal rank (MRR), measuring how well the system finds relevant documents.\n\n* **Generation-focused metrics:** These assess the quality of the generated text. Examples include BLEU, ROUGE, and METEOR, comparing the generated text to reference answers.\n\n* **RAG-specific metrics:** These evaluate the integration of retrieval and generation.  Faithfulness and relevance are mentioned as examples, measuring how well the generated response uses the retrieved information and answers the original query.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.12447429029908899
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 311,
        "candidates_token_count": 146,
        "total_token_count": 457
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:24:17,303 - INFO - Evaluation completed with 6 results
2025-04-08 20:24:17,305 - INFO - Generated metrics reports: reports/rag_evaluation_report_20250408_202417.html
2025-04-08 20:24:17,305 - INFO - Generated metrics report at: {'html': 'reports/rag_evaluation_report_20250408_202417.html'}
2025-04-08 20:49:10,740 - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-04-08 20:49:10,740 - INFO - NumExpr defaulting to 16 threads.
2025-04-08 20:49:10,949 - INFO - PyTorch version 2.3.1 available.
2025-04-08 20:49:12,745 - INFO - Setting up database connection...
2025-04-08 20:49:12,747 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:49:12,747 - INFO - Loading data from sample_data.csv...
2025-04-08 20:49:12,752 - INFO - Loaded 3 chunks from CSV file: sample_data.csv
2025-04-08 20:49:12,753 - INFO - Dataset stored with ID: 67f53e702d8df0af2a97e087
2025-04-08 20:49:12,753 - INFO - Adding chunks to vector store...
2025-04-08 20:49:12,754 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:49:12,757 - INFO - Use pytorch device_name: cuda
2025-04-08 20:49:12,757 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:49:16,251 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-04-08 20:49:16,323 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:49:16,904 - WARNING - Add of existing embedding ID: 0
2025-04-08 20:49:16,904 - WARNING - Add of existing embedding ID: 1
2025-04-08 20:49:16,904 - WARNING - Add of existing embedding ID: 2
2025-04-08 20:49:16,905 - WARNING - Add of existing embedding ID: 0
2025-04-08 20:49:16,905 - WARNING - Add of existing embedding ID: 1
2025-04-08 20:49:16,905 - WARNING - Add of existing embedding ID: 2
2025-04-08 20:49:16,905 - WARNING - Insert of existing embedding ID: 0
2025-04-08 20:49:16,905 - WARNING - Insert of existing embedding ID: 1
2025-04-08 20:49:16,906 - WARNING - Insert of existing embedding ID: 2
2025-04-08 20:49:16,909 - INFO - Chunks successfully added to vector store
2025-04-08 20:49:16,909 - INFO - Generating synthetic QA pairs using Gemini 1.5 Flash...
2025-04-08 20:49:16,911 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 20:49:19,035 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?\nAnswer: RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources, thereby addressing limitations and producing more accurate and informative responses.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.05997518013263571
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 58,
        "total_token_count": 177
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:49:19,035 - INFO - Final extracted text: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?
Answer: RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources, thereby addressing limitations and producing more accurate and informative responses.

2025-04-08 20:49:19,035 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 20:49:19,036 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:49:19,831 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What are vector databases and what types of data do they handle?\nAnswer: Vector databases are specialized database systems designed to efficiently store and query high-dimensional vector embeddings representing semantic information from text, images, or other data types.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.1338760317588339
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 49,
        "total_token_count": 168
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:49:19,832 - INFO - Final extracted text: Question: What are vector databases and what types of data do they handle?
Answer: Vector databases are specialized database systems designed to efficiently store and query high-dimensional vector embeddings representing semantic information from text, images, or other data types.

2025-04-08 20:49:19,832 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 20:49:19,833 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query.
2025-04-08 20:49:21,304 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?\nAnswer: RAG systems are evaluated using retrieval-focused metrics (precision, recall, MRR) to assess document retrieval and generation-focused metrics (BLEU, ROUGE, METEOR) to assess the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance are used to measure how well the generated response uses retrieved information and addresses the query.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.06635537398488899
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 126,
        "candidates_token_count": 95,
        "total_token_count": 221
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:49:21,304 - INFO - Final extracted text: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?
Answer: RAG systems are evaluated using retrieval-focused metrics (precision, recall, MRR) to assess document retrieval and generation-focused metrics (BLEU, ROUGE, METEOR) to assess the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance are used to measure how well the generated response uses retrieved information and addresses the query.

2025-04-08 20:49:21,304 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 20:49:21,306 - INFO - Generated 3 QA pairs
2025-04-08 20:49:21,306 - INFO - Creating test suite...
2025-04-08 20:49:21,306 - INFO - Test suite created with ID: 67f53e792d8df0af2a97e08c
2025-04-08 20:49:21,306 - INFO - Running evaluation with Gemini 1.5 Flash...
2025-04-08 20:49:21,320 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:49:21,326 - INFO - Use pytorch device_name: cuda
2025-04-08 20:49:21,326 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:49:24,809 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:49:24,839 - INFO - Calling generate_answers method
2025-04-08 20:49:24,839 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?

Context: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:49:26,892 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources.  Instead of relying solely on the model's internal knowledge (which may be outdated or incomplete), RAG first retrieves relevant information from a knowledge base, allowing the generative model to produce more accurate and informative responses.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.04523929816025954
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 304,
        "candidates_token_count": 65,
        "total_token_count": 369
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:49:28,506 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:49:28,509 - INFO - Use pytorch device_name: cuda
2025-04-08 20:49:28,509 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:49:31,804 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:49:31,829 - INFO - Calling generate_answers method
2025-04-08 20:49:31,829 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What are vector databases and what types of data do they handle?

Context: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 20:49:33,918 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text, vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings.  These embeddings represent semantic information about text, images, or other data types in a numerical format.  They handle data that has been converted into vector representations, capturing the meaning and relationships between data points.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.0921880765394731
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 298,
        "candidates_token_count": 66,
        "total_token_count": 364
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:49:33,926 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:49:33,928 - INFO - Use pytorch device_name: cuda
2025-04-08 20:49:33,929 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:49:36,815 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:49:36,830 - INFO - Calling generate_answers method
2025-04-08 20:49:36,830 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?

Context: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 20:49:39,102 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Evaluation metrics for RAG systems include:\n\n* **Retrieval-focused metrics:**  precision, recall, and mean reciprocal rank (MRR), assessing the relevance of retrieved documents.\n* **Generation-focused metrics:** BLEU, ROUGE, and METEOR, evaluating the quality of the generated text against reference answers.\n* **RAG-specific metrics:** faithfulness and relevance, measuring how well the generated response uses retrieved information and addresses the query.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.06066813835730919
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 301,
        "candidates_token_count": 91,
        "total_token_count": 392
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 20:49:39,117 - INFO - Evaluation completed with 6 results
2025-04-08 20:49:39,118 - INFO - Generated metrics reports: reports/rag_evaluation_report_20250408_204939.html
2025-04-08 20:49:39,118 - INFO - Generated metrics report at: {'html': 'reports/rag_evaluation_report_20250408_204939.html'}
2025-04-08 20:49:42,502 - INFO - Cleaning up ChromaDB collection...
2025-04-08 20:49:42,504 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:49:42,509 - INFO - Use pytorch device_name: cuda
2025-04-08 20:49:42,509 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 20:49:46,137 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 20:49:46,139 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:49:46,165 - INFO - Successfully cleaned up eval_docs collection.
2025-04-08 20:49:46,165 - INFO - ChromaDB cleanup completed.
2025-04-08 20:50:00,173 - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-04-08 20:50:00,173 - INFO - NumExpr defaulting to 16 threads.
2025-04-08 20:50:00,324 - INFO - PyTorch version 2.3.1 available.
2025-04-08 20:50:55,141 - INFO - Setting up database connection...
2025-04-08 20:50:55,142 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 20:50:55,143 - INFO - Creating collections...
2025-04-08 20:50:55,145 - INFO - Collection 'llms' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'llm_endpoints' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'model_deployments' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'datasets' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'prompt_templates' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'synthetic_data' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'evaluation_harnesses' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'test_suites' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'evaluation_runs' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'test_results' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'metrics' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'metrics_definitions' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Collection 'vector_store' already exists and will be reused.
2025-04-08 20:50:55,145 - INFO - Inserting initial data...
2025-04-08 20:50:55,146 - INFO - LLMs collection already has data, skipping insertion.
2025-04-08 20:50:55,146 - INFO - LLM endpoints collection already has data, skipping insertion.
2025-04-08 20:50:55,147 - INFO - Model deployments collection already has data, skipping insertion.
2025-04-08 20:50:55,147 - INFO - Vector store collection already has data, skipping insertion.
2025-04-08 20:50:55,147 - INFO - Evaluation harnesses collection already has data, skipping insertion.
2025-04-08 20:50:55,148 - INFO - Test suites collection already has data, skipping insertion.
2025-04-08 20:50:55,148 - INFO - Database initialization completed successfully!
2025-04-08 23:47:47,920 - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-04-08 23:47:47,921 - INFO - NumExpr defaulting to 16 threads.
2025-04-08 23:47:48,139 - INFO - PyTorch version 2.3.1 available.
2025-04-08 23:47:50,624 - INFO - Setting up database connection...
2025-04-08 23:47:50,627 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:47:50,627 - INFO - Loading data from sample_data.csv...
2025-04-08 23:47:50,628 - ERROR - Error loading data from CSV: 'utf-8' codec can't decode byte 0x96 in position 2611: invalid start byte
2025-04-08 23:47:50,628 - ERROR - Error storing dataset: 'utf-8' codec can't decode byte 0x96 in position 2611: invalid start byte
2025-04-08 23:47:50,628 - ERROR - Error in main: 'utf-8' codec can't decode byte 0x96 in position 2611: invalid start byte
2025-04-08 23:49:43,366 - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-04-08 23:49:43,367 - INFO - NumExpr defaulting to 16 threads.
2025-04-08 23:49:43,627 - INFO - PyTorch version 2.3.1 available.
2025-04-08 23:49:45,500 - INFO - Setting up database connection...
2025-04-08 23:49:45,502 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:49:45,503 - INFO - Loading data from sample_data.csv...
2025-04-08 23:49:45,508 - INFO - Loaded 6 chunks from CSV file: sample_data.csv using latin1 encoding
2025-04-08 23:49:45,511 - INFO - Dataset stored with ID: 67f568c15914890e8ab32924
2025-04-08 23:49:45,511 - INFO - Adding chunks to vector store...
2025-04-08 23:49:45,512 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:49:45,516 - INFO - Use pytorch device_name: cuda
2025-04-08 23:49:45,516 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:49:49,078 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-04-08 23:49:49,158 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:49:49,756 - INFO - Chunks successfully added to vector store
2025-04-08 23:49:49,756 - INFO - Generating synthetic QA pairs using Gemini 1.5 Flash...
2025-04-08 23:49:49,758 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 23:49:50,708 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?\nAnswer: RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources, retrieved by a separate component, leading to more accurate and informative responses.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.09117312509505475
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 61,
        "total_token_count": 180
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:49:50,708 - INFO - Final extracted text: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?
Answer: RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources, retrieved by a separate component, leading to more accurate and informative responses.

2025-04-08 23:49:50,709 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:49:50,710 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 23:49:51,751 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What are vector databases and what are some examples?\nAnswer: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings representing semantic information.  Examples include Pinecone, Weaviate, Milvus, and ChromaDB.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.03615272261879661
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 55,
        "total_token_count": 174
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:49:51,752 - INFO - Final extracted text: Question: What are vector databases and what are some examples?
Answer: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings representing semantic information.  Examples include Pinecone, Weaviate, Milvus, and ChromaDB.

2025-04-08 23:49:51,752 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:49:51,753 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query.
2025-04-08 23:49:52,848 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?\nAnswer: RAG systems are evaluated using retrieval-focused metrics (e.g., precision, recall, MRR) which assess document retrieval, and generation-focused metrics (e.g., BLEU, ROUGE, METEOR) which assess the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance are used.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.02179202371173435
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 126,
        "candidates_token_count": 90,
        "total_token_count": 216
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:49:52,849 - INFO - Final extracted text: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?
Answer: RAG systems are evaluated using retrieval-focused metrics (e.g., precision, recall, MRR) which assess document retrieval, and generation-focused metrics (e.g., BLEU, ROUGE, METEOR) which assess the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance are used.

2025-04-08 23:49:52,849 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:49:52,851 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made.
2025-04-08 23:49:53,822 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What are the steps involved in opening a savings account according to this Standard Operating Procedure?\nAnswer: The steps include Customer Identification (verifying a government-issued ID), KYC Compliance (filling out the form and checking against the registry), Customer Due Diligence (standard or escalated based on risk), Account Creation (entering details in the CBS), and finally, providing a Welcome Kit & Activation (including initial deposit).\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.19720533034380744
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 208,
        "candidates_token_count": 85,
        "total_token_count": 293
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:49:53,822 - INFO - Final extracted text: Question: What are the steps involved in opening a savings account according to this Standard Operating Procedure?
Answer: The steps include Customer Identification (verifying a government-issued ID), KYC Compliance (filling out the form and checking against the registry), Customer Due Diligence (standard or escalated based on risk), Account Creation (entering details in the CBS), and finally, providing a Welcome Kit & Activation (including initial deposit).

2025-04-08 23:49:53,823 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:49:53,825 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions.
2025-04-08 23:49:54,817 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What are the eligibility requirements for a personal loan in the Retail Banking Division?\nAnswer: Salaried individuals must have a minimum monthly income of \u20b925,000, be aged between 21 and 60 years, and have a credit score above 700.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.11271481360158613
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 200,
        "candidates_token_count": 62,
        "total_token_count": 262
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:49:54,817 - INFO - Final extracted text: Question: What are the eligibility requirements for a personal loan in the Retail Banking Division?
Answer: Salaried individuals must have a minimum monthly income of 25,000, be aged between 21 and 60 years, and have a credit score above 700.

2025-04-08 23:49:54,818 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:49:54,818 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence.
2025-04-08 23:49:55,590 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What is the resolution timeline for ATM dispute resolution?\nAnswer: Disputes must be resolved within 7 working days.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.06726730328339797
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 218,
        "candidates_token_count": 26,
        "total_token_count": 244
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:49:55,591 - INFO - Final extracted text: Question: What is the resolution timeline for ATM dispute resolution?
Answer: Disputes must be resolved within 7 working days.

2025-04-08 23:49:55,591 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:49:55,594 - INFO - Generated 6 QA pairs
2025-04-08 23:49:55,594 - INFO - Creating test suite...
2025-04-08 23:49:55,596 - INFO - Test suite created with ID: 67f568cb5914890e8ab3292c
2025-04-08 23:49:55,596 - INFO - Running evaluation with Gemini 1.5 Flash...
2025-04-08 23:49:55,612 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:49:55,616 - INFO - Use pytorch device_name: cuda
2025-04-08 23:49:55,616 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:49:58,999 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:49:59,028 - INFO - Calling generate_answers method
2025-04-08 23:49:59,028 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?

Context: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 23:50:01,155 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources.  Instead of relying solely on the model's internal knowledge (which may be outdated or incomplete), RAG first retrieves relevant information from a knowledge base, allowing the model to produce more accurate and informative responses.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.034066103398799896
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 304,
        "candidates_token_count": 64,
        "total_token_count": 368
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:50:02,726 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:50:02,729 - INFO - Use pytorch device_name: cuda
2025-04-08 23:50:02,729 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:50:05,321 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:50:05,342 - INFO - Calling generate_answers method
2025-04-08 23:50:05,342 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What are vector databases and what are some examples?

Context: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 23:50:07,372 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings.  These embeddings represent the semantic meaning and relationships of data like text or images in a numerical format.  Examples include Pinecone, Weaviate, Milvus, and ChromaDB.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.027413133989300644
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 295,
        "candidates_token_count": 57,
        "total_token_count": 352
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:50:07,377 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:50:07,380 - INFO - Use pytorch device_name: cuda
2025-04-08 23:50:07,380 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:50:10,494 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:50:10,510 - INFO - Calling generate_answers method
2025-04-08 23:50:10,510 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?

Context: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 23:50:12,836 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Evaluation metrics for RAG systems fall into three categories:\n\n1. **Retrieval-focused metrics:** These assess the quality of the retrieved documents, including precision, recall, and mean reciprocal rank (MRR).\n\n2. **Generation-focused metrics:** These evaluate the quality of the generated text itself, using metrics like BLEU, ROUGE, and METEOR.\n\n3. **RAG-specific metrics:** These measure how well the generated response uses the retrieved information and answers the original query.  Examples include faithfulness and relevance.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.06774137621728059
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 301,
        "candidates_token_count": 107,
        "total_token_count": 408
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:50:12,852 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:50:12,863 - INFO - Use pytorch device_name: cuda
2025-04-08 23:50:12,863 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:50:17,931 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:50:17,947 - INFO - Calling generate_answers method
2025-04-08 23:50:17,947 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What are the steps involved in opening a savings account according to this Standard Operating Procedure?

Context: Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made. Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions. Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence.
2025-04-08 23:50:20,831 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided Standard Operating Procedure for Opening a Savings Account, the steps are:\n\n1. **Customer Identification:** Collect and verify a valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID) against the provided name and address proof.\n\n2. **KYC Compliance:** Ensure the Know Your Customer (KYC) form is completed and cross-checked against the Central KYC Registry.\n\n3. **Customer Due Diligence:** Conduct normal due diligence for low-risk customers. For high-risk customers, escalate to Compliance for enhanced due diligence.\n\n4. **Account Creation:** Enter customer details into the Core Banking System (CBS) and assign a unique Customer ID.\n\n5. **Welcome Kit & Activation:** Provide the account number, cheque book, debit card (if opted for), and set up mobile banking.  The account becomes active upon the initial deposit.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.04850246618082235
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 564,
        "candidates_token_count": 182,
        "total_token_count": 746
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:50:20,851 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:50:20,853 - INFO - Use pytorch device_name: cuda
2025-04-08 23:50:20,853 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:50:23,447 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:50:23,463 - INFO - Calling generate_answers method
2025-04-08 23:50:23,463 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What are the eligibility requirements for a personal loan in the Retail Banking Division?

Context: Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions. Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made. Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence.
2025-04-08 23:50:25,579 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text from \"Personal Loan Disbursement Guidelines (Retail Banking Division),\" the eligibility requirements for a personal loan are:\n\n* **Minimum monthly income:** \u20b925,000 for salaried individuals.\n* **Age:** 21-60 years.\n* **Credit score:** Above 700.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.015872228313499773
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 562,
        "candidates_token_count": 71,
        "total_token_count": 633
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:50:25,587 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:50:25,591 - INFO - Use pytorch device_name: cuda
2025-04-08 23:50:25,591 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:50:28,370 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:50:28,385 - INFO - Calling generate_answers method
2025-04-08 23:50:28,385 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What is the resolution timeline for ATM dispute resolution?

Context: Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence. Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made. Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions.
2025-04-08 23:50:30,283 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text from the \"Procedure for Handling ATM Disputes (Customer Service Manual),\" ATM disputes must be resolved within **7 working days**.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.022219115495681764
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 557,
        "candidates_token_count": 30,
        "total_token_count": 587
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:50:30,300 - INFO - Evaluation completed with 6 results
2025-04-08 23:50:30,304 - INFO - Generated metrics reports: reports/rag_evaluation_report_20250408_235030.html
2025-04-08 23:50:30,304 - INFO - Generated metrics report at: {'html': 'reports/rag_evaluation_report_20250408_235030.html'}
2025-04-08 23:54:26,147 - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-04-08 23:54:26,147 - INFO - NumExpr defaulting to 16 threads.
2025-04-08 23:54:26,283 - INFO - PyTorch version 2.3.1 available.
2025-04-08 23:54:27,526 - INFO - Setting up database connection...
2025-04-08 23:54:27,528 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:54:27,528 - INFO - Loading data from sample_data.csv...
2025-04-08 23:54:27,531 - INFO - Loaded 6 chunks from CSV file: sample_data.csv using latin1 encoding
2025-04-08 23:54:27,532 - INFO - Dataset stored with ID: 67f569dbc9000ad98fad615c
2025-04-08 23:54:27,532 - INFO - Adding chunks to vector store...
2025-04-08 23:54:27,533 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:54:27,535 - INFO - Use pytorch device_name: cuda
2025-04-08 23:54:27,535 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:54:30,345 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-04-08 23:54:30,399 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:54:30,797 - WARNING - Insert of existing embedding ID: 0
2025-04-08 23:54:30,797 - WARNING - Insert of existing embedding ID: 1
2025-04-08 23:54:30,797 - WARNING - Insert of existing embedding ID: 2
2025-04-08 23:54:30,797 - WARNING - Insert of existing embedding ID: 3
2025-04-08 23:54:30,797 - WARNING - Insert of existing embedding ID: 4
2025-04-08 23:54:30,797 - WARNING - Insert of existing embedding ID: 5
2025-04-08 23:54:30,798 - WARNING - Add of existing embedding ID: 0
2025-04-08 23:54:30,798 - WARNING - Add of existing embedding ID: 1
2025-04-08 23:54:30,798 - WARNING - Add of existing embedding ID: 2
2025-04-08 23:54:30,798 - WARNING - Add of existing embedding ID: 3
2025-04-08 23:54:30,798 - WARNING - Add of existing embedding ID: 4
2025-04-08 23:54:30,798 - WARNING - Add of existing embedding ID: 5
2025-04-08 23:54:30,809 - INFO - Chunks successfully added to vector store
2025-04-08 23:54:30,809 - INFO - Generating synthetic QA pairs using Gemini 1.5 Flash...
2025-04-08 23:54:30,811 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 23:54:31,801 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?\nAnswer: RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources, thus addressing limitations and producing more accurate and informative responses.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.02446388992769965
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 58,
        "total_token_count": 177
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:31,801 - INFO - Final extracted text: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?
Answer: RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources, thus addressing limitations and producing more accurate and informative responses.

2025-04-08 23:54:31,802 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:54:31,803 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 23:54:32,900 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What are vector databases and what types of data do they handle?\nAnswer: Vector databases are specialized systems that store and efficiently query high-dimensional vector embeddings representing semantic information from text, images, or other data types.  They capture meaning and relationships in a numerical format.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.11942369362403607
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 58,
        "total_token_count": 177
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:32,901 - INFO - Final extracted text: Question: What are vector databases and what types of data do they handle?
Answer: Vector databases are specialized systems that store and efficiently query high-dimensional vector embeddings representing semantic information from text, images, or other data types.  They capture meaning and relationships in a numerical format.

2025-04-08 23:54:32,901 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:54:32,903 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query.
2025-04-08 23:54:33,953 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?\nAnswer: RAG systems are evaluated using retrieval-focused metrics (e.g., precision, recall, MRR) to assess document retrieval and generation-focused metrics (e.g., BLEU, ROUGE, METEOR) to assess the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance are used.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.031860898050029625
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 126,
        "candidates_token_count": 89,
        "total_token_count": 215
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:33,953 - INFO - Final extracted text: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?
Answer: RAG systems are evaluated using retrieval-focused metrics (e.g., precision, recall, MRR) to assess document retrieval and generation-focused metrics (e.g., BLEU, ROUGE, METEOR) to assess the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance are used.

2025-04-08 23:54:33,953 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:54:33,955 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made.
2025-04-08 23:54:34,795 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What documents are required for customer identification when opening a savings account?\nAnswer: Valid government-issued ID such as Aadhaar, PAN, Passport, or Voter ID, along with address proof.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.10984935079302106
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 208,
        "candidates_token_count": 42,
        "total_token_count": 250
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:34,796 - INFO - Final extracted text: Question: What documents are required for customer identification when opening a savings account?
Answer: Valid government-issued ID such as Aadhaar, PAN, Passport, or Voter ID, along with address proof.

2025-04-08 23:54:34,796 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:54:34,797 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions.
2025-04-08 23:54:35,891 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What are the minimum income requirements and age limits for a personal loan under these guidelines?\nAnswer: Salaried individuals must have a minimum monthly income of \u20b925,000 and be between 21 and 60 years old.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.0993368220779131
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 200,
        "candidates_token_count": 53,
        "total_token_count": 253
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:35,892 - INFO - Final extracted text: Question: What are the minimum income requirements and age limits for a personal loan under these guidelines?
Answer: Salaried individuals must have a minimum monthly income of 25,000 and be between 21 and 60 years old.

2025-04-08 23:54:35,901 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:54:35,902 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence.
2025-04-08 23:54:36,588 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What is the resolution timeline for ATM transaction disputes?\nAnswer: Disputes must be resolved within 7 working days.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.01742553596313183
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 218,
        "candidates_token_count": 26,
        "total_token_count": 244
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:36,588 - INFO - Final extracted text: Question: What is the resolution timeline for ATM transaction disputes?
Answer: Disputes must be resolved within 7 working days.

2025-04-08 23:54:36,588 - INFO - Successfully parsed 1 QA pairs.
2025-04-08 23:54:36,592 - INFO - Generated 6 QA pairs
2025-04-08 23:54:36,592 - INFO - Creating test suite...
2025-04-08 23:54:36,593 - INFO - Test suite created with ID: 67f569e4c9000ad98fad6164
2025-04-08 23:54:36,593 - INFO - Running evaluation with Gemini 1.5 Flash...
2025-04-08 23:54:36,597 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:54:36,600 - INFO - Use pytorch device_name: cuda
2025-04-08 23:54:36,600 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:54:39,410 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:54:39,430 - INFO - Calling generate_answers method
2025-04-08 23:54:39,430 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?

Context: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 23:54:41,516 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources.  Instead of relying solely on the model's internal knowledge (which may be outdated or incomplete), RAG first retrieves relevant information from a knowledge base, allowing the model to produce more accurate and informative responses.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.03115132264792919
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 304,
        "candidates_token_count": 64,
        "total_token_count": 368
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:43,043 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:54:43,046 - INFO - Use pytorch device_name: cuda
2025-04-08 23:54:43,046 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:54:45,662 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:54:45,678 - INFO - Calling generate_answers method
2025-04-08 23:54:45,678 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What are vector databases and what types of data do they handle?

Context: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-08 23:54:47,772 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text, vector databases are specialized database systems that store and efficiently query high-dimensional vector embeddings.  These embeddings are numerical representations of semantic information from various data types, including text and images.  The databases are designed to handle data that can be represented as vectors, capturing the meaning and relationships between data points.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.09692414838876297
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 298,
        "candidates_token_count": 67,
        "total_token_count": 365
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:47,779 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:54:47,781 - INFO - Use pytorch device_name: cuda
2025-04-08 23:54:47,781 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:54:50,771 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:54:50,786 - INFO - Calling generate_answers method
2025-04-08 23:54:50,786 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?

Context: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-08 23:54:51,968 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Evaluation metrics for RAG systems are categorized into retrieval-focused and generation-focused metrics.\n\n**Retrieval-focused metrics** assess the quality of the retrieved documents and include precision, recall, and mean reciprocal rank (MRR).\n\n**Generation-focused metrics** evaluate the quality of the generated text and include BLEU, ROUGE, and METEOR.\n\nFinally, **RAG-specific metrics** such as faithfulness and relevance measure how well the generated response utilizes the retrieved information and answers the original query.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.05861311332852233
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 301,
        "candidates_token_count": 102,
        "total_token_count": 403
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:51,978 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:54:51,980 - INFO - Use pytorch device_name: cuda
2025-04-08 23:54:51,980 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:54:54,636 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:54:54,652 - INFO - Calling generate_answers method
2025-04-08 23:54:54,652 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What documents are required for customer identification when opening a savings account?

Context: Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made. Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions. Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence.
2025-04-08 23:54:56,711 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the \"Standard Operating Procedure for Opening a Savings Account\" document, the required documents for customer identification are a valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID) and a separate address proof.  The name and address on both documents are then verified.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.08745667894007796
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 560,
        "candidates_token_count": 59,
        "total_token_count": 619
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:54:56,718 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:54:56,720 - INFO - Use pytorch device_name: cuda
2025-04-08 23:54:56,720 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:54:59,358 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:54:59,377 - INFO - Calling generate_answers method
2025-04-08 23:54:59,377 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What are the minimum income requirements and age limits for a personal loan under these guidelines?

Context: Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions. Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made. Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence.
2025-04-08 23:55:01,356 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided \"Personal Loan Disbursement Guidelines (Retail Banking Division)\" document, the minimum monthly income requirement for a personal loan is \u20b925,000, and the age limit is 21-60 years.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.02771998882293701
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 564,
        "candidates_token_count": 50,
        "total_token_count": 614
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:55:01,363 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:55:01,365 - INFO - Use pytorch device_name: cuda
2025-04-08 23:55:01,365 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:55:04,383 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:55:04,397 - INFO - Calling generate_answers method
2025-04-08 23:55:04,397 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What is the resolution timeline for ATM transaction disputes?

Context: Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence. Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made. Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions.
2025-04-08 23:55:06,292 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text from the \"Procedure for Handling ATM Disputes (Customer Service Manual),\" ATM transaction disputes must be resolved within **7 working days**.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.03686303861679569
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 557,
        "candidates_token_count": 31,
        "total_token_count": 588
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-08 23:55:06,307 - INFO - Evaluation completed with 6 results
2025-04-08 23:55:06,312 - ERROR - Error generating metrics report: 'actual_answer'
2025-04-08 23:55:06,312 - INFO - Generated metrics report at: None
2025-04-08 23:55:53,691 - INFO - Cleaning up ChromaDB collection...
2025-04-08 23:55:53,694 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:55:53,699 - INFO - Use pytorch device_name: cuda
2025-04-08 23:55:53,699 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-08 23:55:56,921 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-08 23:55:56,922 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-08 23:55:56,950 - INFO - Successfully cleaned up eval_docs collection.
2025-04-08 23:55:56,950 - INFO - ChromaDB cleanup completed.
2025-04-09 00:02:43,276 - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-04-09 00:02:43,276 - INFO - NumExpr defaulting to 16 threads.
2025-04-09 00:02:43,401 - INFO - PyTorch version 2.3.1 available.
2025-04-09 00:02:44,588 - INFO - Setting up database connection...
2025-04-09 00:02:44,590 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-09 00:02:44,590 - INFO - Loading data from sample_data.csv...
2025-04-09 00:02:44,593 - INFO - Loaded 6 chunks from CSV file: sample_data.csv using latin1 encoding
2025-04-09 00:02:44,594 - INFO - Dataset stored with ID: 67f56bcc33481c93cca90f9d
2025-04-09 00:02:44,594 - INFO - Adding chunks to vector store...
2025-04-09 00:02:44,595 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-09 00:02:44,598 - INFO - Use pytorch device_name: cuda
2025-04-09 00:02:44,598 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-09 00:02:47,937 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-04-09 00:02:47,988 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-09 00:02:48,367 - INFO - Chunks successfully added to vector store
2025-04-09 00:02:48,367 - INFO - Generating synthetic QA pairs using Gemini 1.5 Flash...
2025-04-09 00:02:48,369 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-09 00:02:50,583 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?\nAnswer: RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources, addressing limitations through a two-step process of retrieving relevant information and then using a generative model to create a response based on that information.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.07530760120701145
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 74,
        "total_token_count": 193
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:02:50,583 - INFO - Final extracted text: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?
Answer: RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources, addressing limitations through a two-step process of retrieving relevant information and then using a generative model to create a response based on that information.

2025-04-09 00:02:50,583 - INFO - Successfully parsed 1 QA pairs.
2025-04-09 00:02:50,585 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-09 00:02:51,405 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What are vector databases and what are some examples?\nAnswer: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings, representing semantic information in a numerical format.  Examples include Pinecone, Weaviate, Milvus, and ChromaDB.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.06507392724355061
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 119,
        "candidates_token_count": 60,
        "total_token_count": 179
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:02:51,405 - INFO - Final extracted text: Question: What are vector databases and what are some examples?
Answer: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings, representing semantic information in a numerical format.  Examples include Pinecone, Weaviate, Milvus, and ChromaDB.

2025-04-09 00:02:51,405 - INFO - Successfully parsed 1 QA pairs.
2025-04-09 00:02:51,407 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query.
2025-04-09 00:02:52,702 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?\nAnswer: RAG systems are evaluated using retrieval-focused metrics (precision, recall, MRR) to assess document retrieval and generation-focused metrics (BLEU, ROUGE, METEOR) to assess the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance are used.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.06501717037624782
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 126,
        "candidates_token_count": 81,
        "total_token_count": 207
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:02:52,702 - INFO - Final extracted text: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?
Answer: RAG systems are evaluated using retrieval-focused metrics (precision, recall, MRR) to assess document retrieval and generation-focused metrics (BLEU, ROUGE, METEOR) to assess the quality of the generated text.  Additionally, RAG-specific metrics like faithfulness and relevance are used.

2025-04-09 00:02:52,703 - INFO - Successfully parsed 1 QA pairs.
2025-04-09 00:02:52,703 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made.
2025-04-09 00:02:53,442 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What documents are required for customer identification when opening a savings account?\nAnswer: Valid government-issued ID such as Aadhaar, PAN, Passport, or Voter ID are required, along with address proof.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.05102670192718506
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 208,
        "candidates_token_count": 44,
        "total_token_count": 252
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:02:53,443 - INFO - Final extracted text: Question: What documents are required for customer identification when opening a savings account?
Answer: Valid government-issued ID such as Aadhaar, PAN, Passport, or Voter ID are required, along with address proof.

2025-04-09 00:02:53,445 - INFO - Successfully parsed 1 QA pairs.
2025-04-09 00:02:53,445 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions.
2025-04-09 00:02:54,620 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What are the minimum income and age requirements for a personal loan applicant under these guidelines?\nAnswer: The minimum monthly income is \u20b925,000, and the age must be between 21 and 60 years.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.12858926548677332
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 200,
        "candidates_token_count": 51,
        "total_token_count": 251
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:02:54,620 - INFO - Final extracted text: Question: What are the minimum income and age requirements for a personal loan applicant under these guidelines?
Answer: The minimum monthly income is 25,000, and the age must be between 21 and 60 years.

2025-04-09 00:02:54,620 - INFO - Successfully parsed 1 QA pairs.
2025-04-09 00:02:54,621 - INFO - Prompt sent to Gemini model: Generate 1 question-answer pairs from the following text. Format each pair as 'Question: <question>
Answer: <answer>'

Text: Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence.
2025-04-09 00:02:55,231 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Question: What is the resolution timeline for ATM disputes?\nAnswer: Disputes must be resolved within 7 working days.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.09554029464721679
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 218,
        "candidates_token_count": 25,
        "total_token_count": 243
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:02:55,231 - INFO - Final extracted text: Question: What is the resolution timeline for ATM disputes?
Answer: Disputes must be resolved within 7 working days.

2025-04-09 00:02:55,231 - INFO - Successfully parsed 1 QA pairs.
2025-04-09 00:02:55,234 - INFO - Generated 6 QA pairs
2025-04-09 00:02:55,234 - INFO - Creating test suite...
2025-04-09 00:02:55,235 - INFO - Test suite created with ID: 67f56bd733481c93cca90fa5
2025-04-09 00:02:55,236 - INFO - Running evaluation with Gemini 1.5 Flash...
2025-04-09 00:02:55,242 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-09 00:02:55,249 - INFO - Use pytorch device_name: cuda
2025-04-09 00:02:55,249 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-09 00:02:57,941 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-09 00:02:57,971 - INFO - Calling generate_answers method
2025-04-09 00:02:57,971 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: How does the Retrieval-Augmented Generation (RAG) approach improve upon traditional large language models?

Context: The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-09 00:02:59,963 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "RAG improves upon traditional large language models by grounding their generations in external, up-to-date knowledge sources.  Instead of relying solely on the knowledge embedded during the model's training, RAG retrieves relevant information from a knowledge base before generating a response, leading to more accurate and informative outputs.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.07386043866475424
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 304,
        "candidates_token_count": 60,
        "total_token_count": 364
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:03:01,564 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-09 00:03:01,567 - INFO - Use pytorch device_name: cuda
2025-04-09 00:03:01,567 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-09 00:03:04,283 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-09 00:03:04,298 - INFO - Calling generate_answers method
2025-04-09 00:03:04,298 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What are vector databases and what are some examples?

Context: Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector. Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources.
2025-04-09 00:03:06,398 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings.  These embeddings represent the semantic information of data (like text or images) in a numerical format that shows meaning and relationships.  Examples of popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.09830859213164359
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 295,
        "candidates_token_count": 66,
        "total_token_count": 361
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:03:06,410 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-09 00:03:06,413 - INFO - Use pytorch device_name: cuda
2025-04-09 00:03:06,413 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-09 00:03:09,191 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-09 00:03:09,206 - INFO - Calling generate_answers method
2025-04-09 00:03:09,206 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What types of metrics are used to evaluate Retrieval Augmented Generation (RAG) systems?

Context: Evaluation metrics for RAG systems include both retrieval-focused metrics and generation-focused metrics. Retrieval metrics like precision, recall, and mean reciprocal rank (MRR) assess how well the system retrieves relevant documents. Generation metrics like BLEU, ROUGE, and METEOR evaluate the quality of the generated text by comparing it to reference answers. Additionally, RAG-specific metrics such as faithfulness and relevance help measure how well the generated response uses the retrieved information and addresses the original query. The Retrieval-Augmented Generation (RAG) approach combines the strengths of retrieval-based and generation-based methods for natural language processing tasks. In RAG, a retrieval component first fetches relevant documents or passages from a knowledge base, and then a generative model uses this retrieved information to produce more accurate and informative responses. This approach helps address the limitations of large language models by grounding their generations in external, up-to-date knowledge sources. Vector databases are specialized database systems designed to store and efficiently query high-dimensional vector embeddings. These embeddings represent semantic information about text, images, or other data types in a numerical format that captures their meaning and relationships. Popular vector databases include Pinecone, Weaviate, Milvus, and ChromaDB. They typically support similarity search operations like k-nearest neighbors (kNN) to find the most similar vectors to a query vector.
2025-04-09 00:03:11,517 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Evaluation metrics for RAG systems include:\n\n* **Retrieval-focused metrics:** precision, recall, and mean reciprocal rank (MRR), assessing the relevance of retrieved documents.\n* **Generation-focused metrics:** BLEU, ROUGE, and METEOR, evaluating the quality of the generated text compared to reference answers.\n* **RAG-specific metrics:** faithfulness and relevance, measuring how well the generated response uses retrieved information and addresses the original query.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.031748569529989494
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 301,
        "candidates_token_count": 92,
        "total_token_count": 393
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:03:11,531 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-09 00:03:11,543 - INFO - Use pytorch device_name: cuda
2025-04-09 00:03:11,543 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-09 00:03:14,127 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-09 00:03:14,142 - INFO - Calling generate_answers method
2025-04-09 00:03:14,142 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What documents are required for customer identification when opening a savings account?

Context: Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made. Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions. Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence.
2025-04-09 00:03:15,090 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the \"Standard Operating Procedure for Opening a Savings Account\" document, the required documents for customer identification are a valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID) and a separate address proof.  The name and address on both documents are then verified.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.09101190405376887
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 560,
        "candidates_token_count": 59,
        "total_token_count": 619
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:03:15,105 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-09 00:03:15,109 - INFO - Use pytorch device_name: cuda
2025-04-09 00:03:15,109 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-09 00:03:17,680 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-09 00:03:17,696 - INFO - Calling generate_answers method
2025-04-09 00:03:17,696 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What are the minimum income and age requirements for a personal loan applicant under these guidelines?

Context: Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions. Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made. Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence.
2025-04-09 00:03:19,646 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text, the minimum income requirement for a personal loan applicant is \u20b925,000 per month, and the age requirement is between 21 and 60 years old.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.10409945110942043
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 564,
        "candidates_token_count": 43,
        "total_token_count": 607
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:03:19,653 - INFO - Connected to MongoDB: mongodb://localhost:27017/, Database: rag_evaluation
2025-04-09 00:03:19,656 - INFO - Use pytorch device_name: cuda
2025-04-09 00:03:19,656 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-09 00:03:24,032 - INFO - Successfully retrieved existing eval_docs collection.
2025-04-09 00:03:24,046 - INFO - Calling generate_answers method
2025-04-09 00:03:24,046 - INFO - Prompt sent to Gemini model: Answer the following question based on the context: Question: What is the resolution timeline for ATM disputes?

Context: Document Title: Procedure for Handling ATM Disputes (Customer Service Manual)

In case of a failed ATM transaction (e.g., cash not dispensed but amount debited), follow this resolution process:

Complaint Lodgment:

Customer can register a dispute via branch, call center, or mobile app.

Obtain transaction details: date, time, ATM ID, amount.

Initial Acknowledgment:

Send a complaint receipt SMS/email to the customer within 2 hours.

Investigation:

Check ATM logs and switch data.

If the cash was not dispensed, raise a chargeback request with the card network (Visa/Mastercard/RuPay).

Resolution Timeline:

Disputes must be resolved within 7 working days.

Refund must be credited to the customer's account if found valid.

Customer Notification:

Inform customer of the outcome via SMS/email.

Provide a reference number for future correspondence. Document Title: Standard Operating Procedure for Opening a Savings Account
To open a savings account, follow the steps below:
Customer Identification: Collect valid government-issued ID (Aadhaar, PAN, Passport, or Voter ID). Verify against the name and address proof provided.
KYC Compliance: Ensure Know Your Customer (KYC) form is filled. Cross-check against the Central KYC Registry.
Customer Due Diligence:
For low-risk customers: proceed with normal due diligence.
For high-risk customers (e.g., politically exposed persons), escalate to Compliance for enhanced due diligence.
Account Creation: Enter details in the Core Banking System (CBS) and assign a unique Customer ID.
Welcome Kit & Activation: Provide account number, cheque book, debit card (if opted), and mobile banking setup. Account becomes active once the initial deposit is made. Document Title: Personal Loan Disbursement Guidelines (Retail Banking Division)

Eligibility Check:

Salaried individuals must have a minimum monthly income of ?25,000.

Age limit: 2160 years.

Credit score must be above 700.

Document Collection:

Income proof (salary slips or ITR for 2 years)

Address proof

Identity proof

Bank statements (last 6 months)

Loan Sanctioning:

Based on risk profile, tenure, and repayment capacity.

Use the internal Loan Management System (LMS) to evaluate and approve.

Disbursement:

Post-approval, the loan amount is transferred to the applicant's bank account.

Disbursement letter and repayment schedule are issued.

ECS mandate is activated for EMI deductions.
2025-04-09 00:03:24,996 - INFO - Received response from Gemini model: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the provided text from the \"Procedure for Handling ATM Disputes (Customer Service Manual)\", ATM disputes must be resolved within **7 working days**.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "avg_logprobs": -0.06613123019536336
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 556,
        "candidates_token_count": 30,
        "total_token_count": 586
      },
      "model_version": "gemini-1.5-flash"
    }),
)
2025-04-09 00:03:25,003 - INFO - Evaluation completed with 6 results
2025-04-09 00:03:25,006 - INFO - Generated metrics reports: reports/rag_evaluation_report_20250409_000325.html
2025-04-09 00:03:25,006 - INFO - Generated metrics report at: {'html': 'reports/rag_evaluation_report_20250409_000325.html'}
